%!TEX root = ../main.tex

%il faut decrire la structure de notre chapitre 
For the analysis of our algorithms and data structures, we decided to analyze the run time on three types of instances :
\begin{description}
\item[Density variation instances :]{Graphs having a fixed number of vertices ($|V|=1000$) and a density of edges ranging from 5 to 100\%. The maximum capacity of an edge is 10000.}
\item[Size variation instances :]{Graphs having a density of edges fixed (10\%) and a number of vertices varying from $|V|=1000$ to $|V|=5000$. The maximum capacity of an edge is 10000.}
\item[Matching problem instances :]{Graphs having a source $s$, a sink $t$ and two groups of 500 vertices $R1$ and $R2$. 500 edges connect $s$ to all vertices in $R1$ and 500 other edges connect all vertices in $R2$ to $t$. All other possible edges can only go from $R1$ to $R2$. The graphs have a density of connectivity between $R1$ and $R2$ ranging from 5 to 100\%. The capacity of all edges is 1.
This type of graph is often used in the maximum flow problem.

The Figure~\ref{fig:matching} represents a matching problem instance with a density of  connectivity between $R1$ and $R2$ equal to 100\%.

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.3]{images/matchinginstance.png}
\caption{Matching problem instance with a density of edges equal to 100\%.}
\label{fig:matching}
\end{center}
\end{figure}}
\end{description} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%GENERATION%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Instances generation}
To obtain the necessary instances, we implemented instances generators which respects the characteristics of the network graphs (a connected graph where each vertex has at least one incoming and one outgoing edge).

\subsection{Density variation instances}
For the density variation instances, we first create a minimal connected graph. To do this, let $Connected$ be the set of the connected vertices, $DoubleConnected$ the set of vertex having at least one incoming and one outgoing edge, $Edges$ the set of edges and $AllEdges$ the set of all possible edges. Initially, $Connected$ contains the vertex 0, $DoubleConnected$ and $Edges$ are empty and $AllEdges$ contains all possible edges. When we want to add a vertex $v$ to the graph, we take a random vertex $r$ from $Connected$, add the edge ($v$,$r$) in $Edges$, add $v$ in $Connected$, remove the edges ($v$,$r$) and ($r$,$v$) from $AllEdges$ and add $r$ in $DoubleConnected$. After adding our 1000 vertices, we have a connected graph where each vertex has one incoming edge and sometimes at least one outgoing edge (all vertices in $DoubleConnected$).

For each vertex not present in $DoubleConnected$, we take a random vertex from $Connected$, add the edge between them in $Edges$ and remove this edge and its opposite from $AllEdges$. For this step, to avoid adding an edge (or its opposite) which is already present in the graph, we check if the new edge and its opposite are not present in $Edges$. We thus have a connected graph where each vertex has one incoming edge and at least one outgoing edge.

We then add the necessary number of edges to obtain the desired density. We take a random edge from $AllEdges$, add it to $Edges$ and remove it and its opposite from $AllEdges$. A first graph is generated when we have a density of 5\%. We add it edges to obtain a density of 10\% and generate a second graph. And so on up to 100\%. At the end, a density variation instance is composed by twenty graphs where each graph generated before an other one is a sub-graph of the latter. With $|V|=1000$, a complete graph has $\frac{(|V|-1)(|V|)}{2} = 499500$ edges.

The Algorithm \ref{algo:densitygenerator} represents the density variation instance generator.

We generate 10 instances of this type.


\begin{algorithm}
 $Connected$ : set of connected vertices \\
 $DoubleConnected$ : set of vertex having at least one incoming and one outgoing edge \\
 $Edges$ : set of edges on the graph \\
 $AllEdges$ : set of all possible edges on the graph\\
 $Connected \gets \{0\}$, $DoubleConnected\gets \emptyset$, $Edges\gets \emptyset$, $AllEdges\gets \emptyset$\;
 
 \For{$u \gets 0$ \KwTo $|V|-1$}{
 \For{$v \gets 0$ \KwTo $|V|-1$}{
 \If{$u \neq v$}{add ($u$,$v$) in $AllEdges$\;}
 }
 }

 \For{$v \gets 1$ \KwTo $|V|-1$}{
 $r \gets$ random vertex in $Connected$\;
 add ($v$,$r$) to $Edges$\;
 add $v$ in $Connected$\;
 remove ($v$,$r$) and ($r$,$v$) from $AllEdges$\;
 add $r$ in $DoubleConnected$\;
 }

 \For{$v \gets 0$ \KwTo $|V|-1$}{
 \If{!($DoubleConnected$ contains $v$)}{
  $r \gets$ random vertex in $Connected$\;
  \eIf{$Edges$ contains ($v$,$r$) $||$ contains ($r$,$v$)}{$v=v-1$\;}{
  add ($v$,$r$) to $Edges$\;
  remove ($v$,$r$) and ($r$,$v$) from $AllEdges$\;
  add $v$ in $DoubleConnected$\;
   }
 }
 }

 \For{$prct \gets 5$ \KwTo $100$ \textbf{by} $5$}{
 \While{$\frac{|Edges|-(V-1)}{\frac{V\cdot(V-1)}{2}-(V-1)}\cdot 100 < prct$}{$e \gets$ random edge in $AllEdges$\;
 add $e$ to $Edges$\;
 remove $e$ and reverse $e$ from $AllEdges$\;
 }
 generate graph with $Edges$\;
 }
 \caption{Density variation instance generator}
 \label{algo:densitygenerator}
\end{algorithm}

\subsection{Size variation instances}
For the size variation instances, we use the same technique as for the density variation instances without the $AllEdges$ set. Indeed, it is not necessary to generate all possible edges for graphs with a 10\% of edge density. Especially when we know that a complete graph with $|V|=5000$ has 12497500 edges. To know if an edge (or its opposite) is already present in the graph, we check if it is contained in $Edges$.

We generate a network graph with $|V|=1000$ and an edge density of 10\%. We add it 500 vertices and the corresponding edges to respect the characteristics of the network graphs. We add then the necessary edges to keep a 10\% edge density and generate this new graph. And so on until $|V|=5000$. At the end, a size variation instance is composed by nine graphs where each graph generated before an other one is a sub-graph of the latter.

We generate 10 instances of this type.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%PUSH RELABEL%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Push-Relabel}

In this section, we will analyze how the different heuristics performs in the push-relabel algorithm. We also tried each heuristic with and without the height label initialization phase (as explained in Chapter~\ref{improvements}) to show if this phase is useful in practice.
\subsection{Height label initialization}

To analyze the effects of the height label initialization, we decided to compute the number of operations performed. In the preflow-push algorithms, they are three differents operations: the relabelling, the saturing push and the non-saturing push.

\subsubsection{Relabelling}
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.15]{images/meanrelabels.png}
\caption{The average number of relabels in density variation instances. All the algorithms have the same count of relabeling operation.}
\label{fig:mean_relabel}
\end{center}
\end{figure}

We can observe in Figure~\ref{fig:mean_relabel} what we said in Section~\ref{sec:hauteurs} in terms of relabels. The number of relabels is not affected by the fact we initialize or not the height label in the preflow phase of the algorithm. The reason of this is explained in Section~\ref{sec:hauteurs}.

\subsubsection{Saturing push}
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.125]{images/meansaturingpushes.png}
\caption{The mean number of saturing pushes in density variation instances.}
\label{fig:mean_sat}
\end{center}
\end{figure}

We can observe in Figure~\ref{fig:mean_sat} that for each heuristic, the initialization is slightly beneficial in term of the number of saturing pushes. The highest label heuristic perfoms more saturing pushes than the FIFO heuritic, and the generic algorithm performs less saturing pushes than the two others.


\subsubsection{Non-saturing push}

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.125]{images/meannonsaturingpushes.png}
\caption{The mean number of non-saturing pushes in density variation instances.}
\label{fig:mean_non_sat}
\end{center}
\end{figure}

As for the saturing push, the number of non-saturing pushes slightly decreases when we do an initialization of the height labels. The highest label heuristic decreases the number of non-saturing pushes compared to the generic algorithm. The FIFO heuristic tends to augment this number of pushes.

%%TODO remarque francois


\subsection{Best Push-Relabel}

In this section, we will compare the run time of each algorithm on different kinds of instances.

\subsubsection{Density variation instances}
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.65]{images/meandensitypr.png}
\caption{The execution time of the different push relabels by density.}
\label{fig:mean_density_pr}
\end{center}
\end{figure}

We can see here that the \textit{FIFOPushRelabel} is the fastest. For each heuristic, it is interesting to see that the initialization phase does not make the resolution of the problem faster. This is because the computation of the initialization phase does not benefit to resolve the problem faster. 


\subsubsection{Size variation instances}
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.54]{images/meansizepr.png}
\caption{The execution time of the different push relabels by size.}
\label{fig:mean_size_pr}
\end{center}
\end{figure}

When changing the size of the network, we can see that the highest label heuristic performs the best. The initialization of the algorithm does not seem to change a lot the computation time of each heuristic. In this case, the highest label is heuristic faster.
%TODO Analyse?


\subsubsection{Matching problem instances}
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.54]{images/meanmatchingpr.png}
\caption{The execution time of the different push relabels on the matching problem instances.}
\label{fig:mean_matching_pr}
\end{center}
\end{figure}

Here, we can observe that the initialization phase makes each heuristic slower. It is because the problem here is simpler and very quick to solve, thus, the initialization phase takes a more important place in the result. The spike on the graph is caused by the data structures used, the \textit{sparsemap} (Section~\ref{sec:datastructuress}). Here, the high label heuristic is the best as well.

%%TODO remarque
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%DATA STRUCTURES%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Data structures}

\label{sec:datastructuress}
In this section, we would like to determine which data structure is the most adapted for our algorithms. We therefore analyzed experimentally the run time of each algorithm based on 5 data structures defined on the Chapter 3.
\subsection{Push-Relabel}	
The Figure \ref{fig:prmeandensity}, \ref{fig:prmeansize} and \ref{fig:prmeanmatching} represents the average run time of all data structures with Highest Label Push-Relabel. They were computed on, respectively, the density variation, the size variation and the matching problem instances.
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.63]{images/results/prmeandensity.png}
\caption{Average run time of all data structures with Highest Label Push-Relabel on density variation instances.}
\label{fig:prmeandensity}
\end{center}
\end{figure}
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.6]{images/results/prmeansize.png}
\caption{Average run time of all data structures with Highest Label Push-Relabel on size variation instances.}
\label{fig:prmeansize}
\end{center}
\end{figure}
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.6]{images/results/prmeanmatching.png}
\caption{Average run time of all data structures with Highest Label Push-Relabel on matching problem instances.}
\label{fig:prmeanmatching}
\end{center}
\end{figure}

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.55]{images/results/prmeanmatching2.png}
\caption{Average run time of all data structures without the $hashmap$ and the $treemap$ with Highest Label Push-Relabel on matching problem instances.}
\label{fig:prmeanmatching2}
\end{center}
\end{figure}
On the 3 type of instances, we notice that the $hashmap$ and the $treemap$ offer very poor performances. For density and size variation instances, structures based on the $sparse set$ stand out from others by their good performances. The $linked list$ being between map based structures and  $sparse set$ based structures. To be able to decide between the $split array$ and the $sparse map$, we need to look at the Figure \ref{fig:prmeanmatching2}, which represents the average run time of the $split array$, the $sparse map$ and the $linked list$ on matching problem instances.

The most adapted data structure for Push-Relabel algorithms is the $split array$.

\subsubsection{Profiler}
We have profiled our code with all data structure on a complete density instance graph with Highest Label Push-Relabel. The Figure~\ref{fig:pradja} represents the number of invocation and the total run time of the function $getAdjacents$, which is the function that take the most time. Those results highlight the good performances of the $split array$ and the $sparse map$. The total time of the function $parse$, which parse the text file to the data structure, allows us to understand why the $split array$ is better than the $sparse map$. Indeed, the function $parse$ of the $sparse map$ take 1143 ms while the $split array$ take 404 ms. 

\begin{figure}[H]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
	\hline
     & \textbf{Hash map} & \textbf{Tree map} & \textbf{Simple linked list} & \textbf{Split array} & \textbf{Sparse map}\\
     \hline	
   getAdjacents & $21.940$ & $21.940$ & $17.555$ & $18.980$ & $19.513$ \\
   total time (ms) & $208$ & $311$ & $120$ & $80$ & $30$ \\
   \hline
\end{tabular}
\caption{The number of invocation of the function $getAdjacents$ and its total time with Highest Label Push Relabel.}
\label{fig:pradja}
\end{figure}

\subsection{Edmonds-Karp}
The Figure~\ref{fig:ekmeandensity}, \ref{fig:ekmeansize} and \ref{fig:ekmeanmatching} represents the average run time of all data structures with Edmonds-Karp. They were computed on, respectively, the density variation, the size variation and the matching problem instances.
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.54]{images/results/ekmeandensity.png}
\caption{Average run time of all data structures with Edmonds-Karp on density variation instances.}
\label{fig:ekmeandensity}
\end{center}
\end{figure}
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.54]{images/results/ekmeansize.png}
\caption{Average run time of all data structures with Edmonds-Karp on size variation instances.}
\label{fig:ekmeansize}
\end{center}
\end{figure}
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.5]{images/results/ekmeanmatching.png}
\caption{Average run time of all data structures with Edmonds-Karp on matching problem instances.}
\label{fig:ekmeanmatching}
\end{center}
\end{figure}
As for the Push-Relabel, the structures based on maps are the least efficient while the structures based on the $sparse set$ provide better performances than the others. Nevertheless, the difference between the $split array$ and the $sparse map$ is more pronounced. 
We can conclude that the most appropriate data structure for Edmonds-Karp is the $split array$.
\subsubsection{Profiler}
After profiled our code on a complete density variation graph, several observations can be made. First, we explain the poor performances of the $hashmap$ and the $treemap$ thanks to the Figure~\ref{fig:ekadja} which represents the number of invocations of the function $getAdjacents$.
\begin{figure}[h]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
	\hline
     & \textbf{Hash map} & \textbf{Tree map} & \textbf{Simple linked list} & \textbf{Split array} & \textbf{Sparse map}\\
     \hline	
   getAdjacents & $217.142$ & $426.317$ & $31.465$ & $56.844$ & $184.623$ \\
   total time (ms) & $1.933$ & $7.714$ & $322$ & $163$ & $1.092$ \\
   \hline
\end{tabular}
\caption{The number of invocation of the function $getAdjacents$ and its total time with Edmonds-Karp.}
\label{fig:ekadja} 
\end{figure}
The $linkedlist$ is slower than the $sparsemap$ and the $splitarray$ because its function $getCapacity$ is very slow. That is what we can see in the Figure~\ref{fig:ekcapa} which represents the number of invocation and the total time of the function $getCapacity$.
\begin{figure}[h]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
	\hline
     & \textbf{Simple linked list} & \textbf{Split array} & \textbf{Sparse map}\\
     \hline	
   getCapacity & $220.029$ & $279.480$ & $783.293$ \\
   total time (ms) & $271$ & $90$ & $32$ \\
   \hline
\end{tabular}
\caption{The number of invocation of the function $getCapacity$ and its total time with Edmonds-Karp.}
\label{fig:ekcapa}
\end{figure}
\subsection{Ford-Fulkerson with scaling}
The Figure~\ref{fig:ffmeandensity}, \ref{fig:ffmeansize} and \ref{fig:ffmeanmatching} represents the average run time of all data structures with Ford-Fulkerson with scaling. They were computed on, respectively, the density variation, the size variation and the matching problem instances.
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.5]{images/results/ffmeandensity.png}
\caption{Average run time of all data structures with Ford-Fulkerson with scaling on density variation instances.}
\label{fig:ffmeandensity}
\end{center}
\end{figure}
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.5]{images/results/ffmeansize.png}
\caption{Average run time of all data structures with Ford-Fulkerson with scaling on size variation instances.}
\label{fig:ffmeansize}
\end{center}
\end{figure}
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.55]{images/results/ffmeanmatching.png}
\caption{Average run time of all data structures with Ford-Fulkerson with scaling on matching problem instances.}
\label{fig:ffmeanmatching}
\end{center}
\end{figure}


Those figures highlight the poor performance of the $linked list$ while the $hash map$ seems to be most appropriated for Ford-Fulkerson with scaling than for the other algorithms. We nevertheless note that the map based structures explode with a high density of edges for the matching problem instances. The structure based on the $sparse set$ offer, as always, good performances. 

The $sparse map$ is the most adapted data structure for Ford-Fulkerson with scaling.

\subsubsection{Profiler}
When we look to the Figure~\ref{fig:ffcapa}, which represents the total time of the function $getCapacity$ and its number of invocations, we understand why the $linkedlist$ is so slow with Ford-Fulkerson with scaling. This figure explains also why the $split array$ and the $treemap$ are not well adapted to this algorithm, its function $getCapacity$ is too slow.

The $hashmap$ has a very fast function $getCapacity$ but its function $getAdjacents$ take too much time compared to the $sparsemap$ (1146 ms for the $hashmap$ and 31 ms for the $sparsemap$). This is why the $sparse map$ is the most adapted data structure for Ford-Fulkerson with scaling.

\begin{figure}[H]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
	\hline
     & \textbf{Hash map} & \textbf{Tree map} & \textbf{Simple linked list} & \textbf{Split array} & \textbf{Sparse map}\\
     \hline	
   getCapacity & $58.718.016$ & $58.493.923$ & $6.319.627$ & $7.515.168$ & $7.604.521$ \\
   total time (ms) & $1$ & $2.020$ & $7.489$ & $1.411$ & $209$ \\
   \hline
\end{tabular}
\caption{The number of invocation of the function $getCapacity$ and its total time with Ford-Fulkerson with scaling.}
\label{fig:ffcapa} 
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%BEHAVIORS%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Behaviors}
\subsection{Edmonds-Karp}
\subsubsection{Density variation instances}
One of the most blatant observations on Edmonds-Karp is that it is very regular, what we can observe in the Figure~\ref{fig:EKmean}, which represents the run time on each density variation instance, with its best data structure, the $split array$. Indeed, Edmonds-Karp solves the maximum flow problem on complete graphs with $|V|=1000$ with a run time ranging from 750 to 1000 ms.
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.6]{images/results/EKmean.png}
\caption{Run time of Edmonds-Karp on all density variation instances with the $split array$.}
\label{fig:EKmean}
\end{center}
\end{figure}
\subsubsection{Size variation instances}
The Figure~\ref{fig:EKmeansize} represents the run time of Edmonds-Karp on all size variation instances with the $split array$. Edmonds-Karp is regular with a run time ranging from 750 to 1000 ms to solve the maximum flow problem on graphs with $|V|=5000$ and a density of edges equal to 10\% ($|E|=1249750$).
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.55]{images/results/EKmeansize2.png}
\caption{Run time of Edmonds-Karp on all size variation instances with the $split array$.}
\label{fig:EKmeansize}
\end{center}
\end{figure}
\subsubsection{Matching problem instances}
As usual, Edmonds-Karp remains very regular. We can see that on the Figure~\ref{fig:ekmatching} which represents the run time of Edmonds-Karp on all matching problem instances. It takes an average of 460ms to solve a matching problem instance with a maximum density of edges.
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.55]{images/results/ekmatching.png}
\caption{Run time of Edmonds-Karp on all matching problem instances with the $split array$.}
\label{fig:ekmatching}
\end{center}
\end{figure}


\subsection{Ford-Fulkerson with scaling}
\subsubsection{Density variation instances}
The Figure~\ref{fig:FFmean} represents the run time on each density variation instance, with the $sparsema$p. Although less regular than Edmonds-Karp, Ford-Fulkerson with scaling remains stable with a run time ranging from 500 to 2500 ms to solve the maximum flow problem on complete graphs with $|V|=1000$.
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.5]{images/results/FFmean.png}
\caption{Run time of Ford-Fulkerson with scaling on all density variation instances with the $sparse map$.}
\label{fig:FFmean}
\end{center}
\end{figure}

This graph also highlights the presence of two type of graphs on this type of instances. Indeed, we observe that the run time of instances 2, 4, 9 and 10 are similar. The same clustering is observable with the instances 1, 5 and 7.
\subsubsection{Size variation instances}
As we can see on the Figure~\ref{fig:FFmeansize} which represents the run time of Ford-Fulkerson with scaling on all size variation instances, it is quite regular but its performances are slightly worse than Edmonds-Karp. Indeed, Ford-Fulkerson with scaling solve the maximum flow problem on graphs with $|V|=5000$ and a density of edges equal to 10\% with a run time ranging from 3000 to 4000 ms.
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.5]{images/results/FFmeansize2.png}
\caption{Run time of Ford-Fulkerson with scaling on all size variation instances with the $sparse map$.}
\label{fig:FFmeansize}
\end{center}
\end{figure}
\subsubsection{Matching problem instances}
The Figure~\ref{fig:ffmatching} show that Ford-Fulkerson with scaling has a constant run time to solve this kind of instance. The spike present at the end of the graph is due to its data structure. It takes an average of 120ms to solve a matching problem instance with a maximum density of edges.
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.5]{images/results/ffmatching.png}
\caption{Run time of Ford-Fulkerson with scaling on all matching problem instances with the $sparse map$.}
\label{fig:ffmatching}
\end{center}
\end{figure}


\subsection{Push-Relabel}
\subsubsection{Density variation instances}
When we look at the results of the FIFO Push-Relabel, an observation is quite obvious : it is not regular at all. As shown in Figure~\ref{fig:PR6}, the run time on one density variation instance can vary from less than 100 ms to more than 5000 ms.
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.45]{images/results/pri68.png}
\caption{Run time of FIFO Push-Relabel on the density variation instance 6 and 8 with the $split array$.}
\label{fig:PR6}
\end{center}
\end{figure}
It may nevertheless have a stable run time but once very high and once extremely low. That is what we can observe in Figure~\ref{fig:PR10} and Figure~\ref{fig:PR7}.
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.45]{images/results/pri10.png}
\caption{Run time of FIFO Push-Relabel on the density variation instance 10 with the $split array$.}
\label{fig:PR10}
\end{center}
\end{figure}
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.45]{images/results/pri7.png}
\caption{Run time of FIFO Push-Relabel on the density variation instance 7 with the $split array$.}
\label{fig:PR7}
\end{center}
\end{figure}
When displaying the run time of each density variation instance with its best data structure, we obtain a very disparate graph, as we can see in Figure~\ref{fig:PRmean}. Indeed, the FIFO Push-Relabel solve the maximum flow problem on complete graphs with $|V|=1000$ with a run time ranging from 100 ms to 6700 ms.
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.45]{images/results/PRmean.png}
\caption{Run time of FIFO Push-Relabel on all density variation instances with the $split array$.}
\label{fig:PRmean}
\end{center}
\end{figure}

As for Ford-Fulkerson with scaling with this type on instances, we note that the instance 2, 4, 9 and 10 have high and similar run time.
\subsubsection{Size variation instances}
Contrary to the results obtained on the density variation instances, the Highest Label Push-Relabel is regular but it offers catastrophic performances. Indeed, it has a run time ranging from 85000 to 125000 ms to solve the maximum flow problem on graphs with $|V|=5000$ and a density of edges equal to 10\%. It is 1000 times slower than Edmonds-Karp.
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.44]{images/results/HLPRmean.png}
\caption{Run time of Highest Label Push-Relabel on all size variation instances with the $split array$.}
\label{fig:HLPRmean}
\end{center}
\end{figure}
\subsubsection{Matching problem instances}
The Highest Label Push-Relabel excels in this type of instances. Indeed, it has a regular run time ranging from 20 to 30 ms to solve a matching problem instance with a maximum density of edges.
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.44]{images/results/prmatching.png}
\caption{Run time of Highest Label Push-Relabel on all matching problem instances with the $split array$.}
\label{fig:prmatching}
\end{center}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%COMPARAISON%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Comparison}


\subsection{Density variation instances}
The Figure~\ref{fig:MeanInstances} represents, for each algorithm, the average run time on all density variation instances with its best data structure. As we can see, Edmonds-Karp seems to be the most appropriated algorithm to solve the maximum flow problem with graphs having a high-density of edges. \\

The complexity of FIFO Push-Relabel ($O(|V|^3)$) depend less on the number of edges than the complexity of Edmonds-Karp ($O(|V|\cdot |E|^2)$) or Ford-Fulkerson with scaling ($O(|E|^2 \cdot log(U))$). It should thus have the best performances but unfortunately, due to its irregularity, its average run time is high. \\

We believe that these results are due to two things. First, the worst case graph for Edmonds-Karp (a graph where each edge is used in each augmenting path of a different length), which defined its O complexity, is not possible to obtain with the generation of a density variation instance. Its complexity is a loose bound on this kind of graphs. Secondly, we saw that the Push-Relabel has a drawback, the ping pong effect. We think that the density variation instances, due to their random aspect, are favorable to the appearance of structure which allow the ping pong effect.

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.6]{images/meandensity.png}
\caption{Average run time of the three algorithms on all density variation instances.}
\label{fig:MeanInstances}
\end{center}
\end{figure}




\subsection{Size variation instances}
The Figure~\ref{fig:MeanInstancesize} represents, for each algorithm, the average run time on all size variation instances with its best data structure. In this figure, it is clear that the augmenting path algorithms offer incomparable performances with Highest Label Push-Relabel. Edmonds-Karp is the best algorithm to solve the maximum flow problem on graphs with low density of edges and a large number of vertices.

It can be explained by their complexity. Indeed, the preflow-push algorithms ($O(|V|^2 \cdot \sqrt{|E|}$ for the Highest Label heuristic) are more dependant on the number of vertices than the augmenting path algorithms ($O(|V| \cdot |E|^2)$ for Edmonds-Karp).

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.6]{images/meansize.png}
\caption{Average run time of the three algorithms on all size variation instances.}
\label{fig:MeanInstancesize}
\end{center}
\end{figure}




\subsection{Matching problem instances}
The Figure~\ref{fig:meanmatchingproblem} represents, for each algorithm, the average run time on all matching problem instances with its best data structure. We notice that the run time of Highest Label Push-Relabel remains constant during the addition of edges while the run time of Edmonds-Karp increases. It is a logical behaviour in view of the complexities.

When we analyze the structure of this type of graphs, we notice the Edmonds-Karp is not adapted. Indeed, it will look for a huge number of augmenting path due to the backward edges from $R2$ to $R1$ created after a push on edge from $R1$ to $R2$. While Highest Label Push-Relabel visits once each vertex, not taking into account the number of edges.

Highest Label Push-Relabel is undoubtedly the most appropriated algorithm to solve this type of instance. 

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.6]{images/meanmatching.png}
\caption{Average run time of the three algorithms on all matching problem instances.}
\label{fig:meanmatchingproblem}
\end{center}
\end{figure}
